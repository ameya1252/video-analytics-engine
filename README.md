# Distributed Real-Time Video Analytics Engine (C++ Â· AWS Â· Kubernetes Â· WebSocket)

[ðŸ“„ Comprehensive Research Paper](https://drive.google.com/file/d/1TKZ9pRNR-gJAGqyF3-qeXk4XfefEdTfN/view?usp=sharing)

**Highlights**
- Designed for **5,000+ streams**, target **<40â€¯ms gateway latency** and **99.9% uptime** via Kubernetes on AWS.
- GPU-accelerated inference with **TensorRT/CUDA** (stubbed interface + CPU fallback), demonstrating **~3Ã— throughput potential** and **~35% cost reduction** approaches via batching & mixed precision.
- Modular C++ services: **Gateway (WebSocket)**, **Dispatcher**, **Inference**, **Ingest (simulated)**.
- Dockerized + K8s manifests (GPU scheduling), GitHub Actions CI, and Terraform *skeleton* for EKS.

> This repo is designed to *build and run as a demo* even without a GPU/TensorRT by using lightweight stubs.
> Swap the stubs with your TensorRT engine to run on real video streams.

---

## Architecture

```
[Clients / Cameras] â†’ (WebSocket) â†’ [Gateway] â†’ (HTTP/JSON) â†’ [Dispatcher] â†’ [Inference Pods (GPU)]
                                              â†‘                                 â†“
                                      [Ingest Simulator] â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â†’ /infer
```

- **Gateway (C++/Boost.Beast)**: Terminate WebSocket, minimal per-frame overhead, forwards metadata to Dispatcher.
- **Dispatcher (C++)**: Consistent hashing of `stream_id` to an inference backend; health checks; backpressure.
- **Inference (C++)**: HTTP service; stubbed TensorRT interface (compile-time off by default). Returns toy detections.
- **Ingest (C++)**: Optional generator that simulates RTSP frames and pushes JSON frames to the Gateway via WS.
- **Client (web)**: Tiny HTML viewer to open a WS and observe telemetry.

## Quick Start (Docker Compose â€“ Local Demo)

```bash
docker compose up --build
# Gateway: ws://localhost:8080/ws
# Dispatcher: http://localhost:8090
# Inference: http://localhost:8091/infer
```

Open `client/web/index.html` in your browser and click **Connect**.

## Build (CMake)

```bash
mkdir -p build && cd build
cmake -DCMAKE_BUILD_TYPE=Release ..
cmake --build . -j
```

### Dependencies (host build)
- CMake â‰¥ 3.18
- Compiler with C++20
- Boost (system, thread, beast) â€” `libboost-all-dev` on Ubuntu
- (Optional) CUDA + TensorRT if enabling GPU inference

## Kubernetes on AWS (EKS)

- Manifests under `k8s/` (HPA, GPU scheduling via `nvidia.com/gpu` resource request).
- Deploy script: `scripts/deploy_k8s.sh`.
- Terraform *skeleton* under `terraform/` to spin up EKS â€” fill in vars.

> For GPU nodes, install the **NVIDIA device plugin** DaemonSet and use the `inference-deploy.yaml` which requests `nvidia.com/gpu: 1`.

## CI/CD (GitHub Actions â†’ ECR)

Workflow in `.github/workflows/ci.yaml` builds Docker images, pushes to ECR, and (optionally) deploys to the cluster.
Populate repo secrets: `AWS_REGION`, `AWS_ACCOUNT_ID`, `ECR_REPO_PREFIX`, `KUBE_CONFIG` (base64).

## Folder Structure

```
video-analytics-engine/
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/          # shared utils
â”‚   â”œâ”€â”€ gateway/         # WebSocket server
â”‚   â”œâ”€â”€ dispatcher/      # routing/health
â”‚   â”œâ”€â”€ inference/       # HTTP inference (TRT stub + CPU fallback)
â”‚   â””â”€â”€ ingest/          # simulated frame source â†’ WS
â”œâ”€â”€ docker/
â”œâ”€â”€ k8s/
â”œâ”€â”€ client/web/
â”œâ”€â”€ scripts/
â”œâ”€â”€ .github/workflows/
â””â”€â”€ terraform/
```

## Notes / Disclaimers
- WebSocket frame bodies here are JSON metadata for simplicity; you can swap in binary frames (e.g., encoded JPEG/RAW) to optimize throughput.
- Latency numbers are targets; actual performance depends on hardware and network. The design choices (zero-copy paths, batching, mixed precision, pinning, NUMA-aware thread pools) are representative of techniques used to achieve **<40ms gateway latency** and strong uptime SLOs.
